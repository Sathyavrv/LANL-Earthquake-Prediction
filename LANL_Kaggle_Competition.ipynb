{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11000,
          "databundleVersionId": 875412,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 25114,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **LANL Earthquake Prediction**\n",
        "\n",
        "Forecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: when the event will occur, where it will occur, and how large it will be.\n",
        "\n",
        "In this competition, you will address when the earthquake will take place. Specifically, youâ€™ll predict the time remaining before laboratory earthquakes occur from real-time seismic data.\n",
        "\n",
        "Link to the competition : https://www.kaggle.com/competitions/LANL-Earthquake-Prediction\n",
        "\n",
        "by [Sathya Narayanan](https://github.com/Sathyavrv)\n",
        "\n",
        "#### Goal of the Competition\n",
        "\n",
        "The goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes. The data comes from a well-known experimental set-up used to study earthquake physics. The acoustic_data input signal is used to predict the time remaining before the next laboratory earthquake (time_to_failure)."
      ],
      "metadata": {
        "id": "j1dAd6qz3EsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=center>\n",
        "<img src=\"https://www.kaggle.com/competitions/11000/images/header\" width=\"80%\" align=\"center\"></p>"
      ],
      "metadata": {
        "id": "t3W8yzwj3GJt"
      }
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = ':https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F11000%2F875412%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240403%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240403T072813Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5cb36ceb95bd8ff6ecf182eb908e2282f227ec0905a69bf896d04c4a4730af40fe3852cac2acde2ddeb0c946e92dfa5bd826fd395668378f56ce39239c7267f1821e5f73242d3c8fab26df849bbf52ba3a5000ea532e2c6cca97c83b9a7958663375ad08b6275b434a399209c73901e685ea081fb4512710800e6043150fd2ed1e169cdebe75d0e7201b7cbe786fe8f0b9edd9b21c80878d0c5748c316aee0255c6c384a663056667b3427109dad219ff4ac6f5f958df6cfdac77bec58fca5abf262637de51d8b4bb8e21699c3d5b5ebfea3c2bb8614cbddcb583558ad10e25319c9475bb470aaa2fd5690dc0dc714729cad8f60090608fa06b14ce02c4094b5'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "jBkNHRns28cU"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "raw = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-03T07:08:17.404664Z",
          "iopub.execute_input": "2024-04-03T07:08:17.404968Z",
          "iopub.status.idle": "2024-04-03T07:12:19.251559Z",
          "shell.execute_reply.started": "2024-04-03T07:08:17.404906Z",
          "shell.execute_reply": "2024-04-03T07:12:19.250718Z"
        },
        "trusted": true,
        "id": "wmfK6weZ28cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tsfresh.feature_extraction import feature_calculators\n",
        "import librosa\n",
        "import pywt\n",
        "\n",
        "\n",
        "np.random.seed(1337)\n",
        "noise = np.random.normal(0, 0.5, 150_000)\n",
        "\n",
        "\n",
        "def denoise_signal_simple(x, wavelet='db4', level=1):\n",
        "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
        "    #univeral threshold\n",
        "    uthresh = 10\n",
        "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
        "    # Reconstruct the signal using the thresholded coefficients\n",
        "    return pywt.waverec(coeff, wavelet, mode='per')\n",
        "\n",
        "\n",
        "def feature_gen(z):\n",
        "    X = pd.DataFrame(index=[0], dtype=np.float64)\n",
        "\n",
        "    z = z + noise\n",
        "    z = z - np.median(z)\n",
        "\n",
        "    den_sample_simple = denoise_signal_simple(z)\n",
        "    mfcc = librosa.feature.mfcc(z)\n",
        "    mfcc_mean = mfcc.mean(axis=1)\n",
        "    percentile_roll50_std_20 = np.percentile(pd.Series(z).rolling(50).std().dropna().values, 20)\n",
        "\n",
        "    X['var_num_peaks_2_denoise_simple'] = feature_calculators.number_peaks(den_sample_simple, 2)\n",
        "    X['var_percentile_roll50_std_20'] = percentile_roll50_std_20\n",
        "    X['var_mfcc_mean18'] = mfcc_mean[18]\n",
        "    X['var_mfcc_mean4'] = mfcc_mean[4]\n",
        "\n",
        "    return X\n",
        "\n"
      ],
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.status.busy": "2024-04-03T07:12:19.252997Z",
          "iopub.execute_input": "2024-04-03T07:12:19.253219Z",
          "iopub.status.idle": "2024-04-03T07:12:21.23885Z",
          "shell.execute_reply.started": "2024-04-03T07:12:19.253181Z",
          "shell.execute_reply": "2024-04-03T07:12:21.238048Z"
        },
        "trusted": true,
        "id": "0KUhSNEW28cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Parallel, delayed\n",
        "import scipy as sp\n",
        "import itertools\n",
        "import gc\n",
        "\n",
        "def parse_sample(sample, start):\n",
        "    delta = feature_gen(sample['acoustic_data'].values)\n",
        "    delta['start'] = start\n",
        "    delta['target'] = sample['time_to_failure'].values[-1]\n",
        "    return delta\n",
        "\n",
        "def sample_train_gen(df, segment_size=150_000, indices_to_calculate=[0]):\n",
        "    result = Parallel(n_jobs=4, temp_folder=\"/tmp\", max_nbytes=None, backend=\"multiprocessing\")(delayed(parse_sample)(df[int(i) : int(i) + segment_size], int(i))\n",
        "                                                                                                for i in tqdm(indices_to_calculate))\n",
        "    data = [r.values for r in result]\n",
        "    data = np.vstack(data)\n",
        "    X = pd.DataFrame(data, columns=result[0].columns)\n",
        "    X = X.sort_values(\"start\")\n",
        "    return X\n",
        "\n",
        "def parse_sample_test(seg_id):\n",
        "    sample = pd.read_csv('../input/test/' + seg_id + '.csv', dtype={'acoustic_data': np.int32})\n",
        "    delta = feature_gen(sample['acoustic_data'].values)\n",
        "    delta['seg_id'] = seg_id\n",
        "    return delta\n",
        "\n",
        "def sample_test_gen():\n",
        "    X = pd.DataFrame()\n",
        "    submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
        "    result = Parallel(n_jobs=4, temp_folder=\"/tmp\", max_nbytes=None, backend=\"multiprocessing\")(delayed(parse_sample_test)(seg_id) for seg_id in tqdm(submission.index))\n",
        "    data = [r.values for r in result]\n",
        "    data = np.vstack(data)\n",
        "    X = pd.DataFrame(data, columns=result[0].columns)\n",
        "    return X\n",
        "\n",
        "indices_to_calculate = raw.index.values[::150_000][:-1]\n",
        "\n",
        "train = sample_train_gen(raw, indices_to_calculate=indices_to_calculate)\n",
        "gc.collect()\n",
        "test = sample_test_gen()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T07:12:33.947355Z",
          "iopub.execute_input": "2024-04-03T07:12:33.947683Z",
          "iopub.status.idle": "2024-04-03T07:19:35.822818Z",
          "shell.execute_reply.started": "2024-04-03T07:12:33.947622Z",
          "shell.execute_reply": "2024-04-03T07:19:35.822053Z"
        },
        "trusted": true,
        "id": "rPNF0dCk28cX",
        "outputId": "e4f4fffb-96dd-4e17-cba1-826c7b6dfb53",
        "colab": {
          "referenced_widgets": [
            "e03665496e084c3181952d675f27beb8",
            "a5439a869eab47c39292426aa6d9325a"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, max=4194), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e03665496e084c3181952d675f27beb8"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.6/site-packages/pywt/_multilevel.py:148: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  a = a[[slice(s) for s in d.shape]]\n/opt/conda/lib/python3.6/site-packages/pywt/_multilevel.py:148: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  a = a[[slice(s) for s in d.shape]]\n/opt/conda/lib/python3.6/site-packages/pywt/_multilevel.py:148: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  a = a[[slice(s) for s in d.shape]]\n/opt/conda/lib/python3.6/site-packages/pywt/_multilevel.py:148: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  a = a[[slice(s) for s in d.shape]]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "HBox(children=(IntProgress(value=0, max=2624), HTML(value='')))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5439a869eab47c39292426aa6d9325a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.6/site-packages/pywt/_multilevel.py:148: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  a = a[[slice(s) for s in d.shape]]\n/opt/conda/lib/python3.6/site-packages/pywt/_multilevel.py:148: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  a = a[[slice(s) for s in d.shape]]\n/opt/conda/lib/python3.6/site-packages/pywt/_multilevel.py:148: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  a = a[[slice(s) for s in d.shape]]\n/opt/conda/lib/python3.6/site-packages/pywt/_multilevel.py:148: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  a = a[[slice(s) for s in d.shape]]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "etq_meta = [\n",
        "{\"start\":0,         \"end\":5656574},\n",
        "{\"start\":5656574,   \"end\":50085878},\n",
        "{\"start\":50085878,  \"end\":104677356},\n",
        "{\"start\":104677356, \"end\":138772453},\n",
        "{\"start\":138772453, \"end\":187641820},\n",
        "{\"start\":187641820, \"end\":218652630},\n",
        "{\"start\":218652630, \"end\":245829585},\n",
        "{\"start\":245829585, \"end\":307838917},\n",
        "{\"start\":307838917, \"end\":338276287},\n",
        "{\"start\":338276287, \"end\":375377848},\n",
        "{\"start\":375377848, \"end\":419368880},\n",
        "{\"start\":419368880, \"end\":461811623},\n",
        "{\"start\":461811623, \"end\":495800225},\n",
        "{\"start\":495800225, \"end\":528777115},\n",
        "{\"start\":528777115, \"end\":585568144},\n",
        "{\"start\":585568144, \"end\":621985673},\n",
        "{\"start\":621985673, \"end\":629145480},\n",
        "]\n",
        "\n",
        "for i, etq in enumerate(etq_meta):\n",
        "    train.loc[(train['start'] + 150_000 >= etq[\"start\"]) & (train['start'] <= etq[\"end\"] - 150_000), \"eq\"] = i\n",
        "\n",
        "train_sample = train[train[\"eq\"].isin([2, 7, 0, 4, 11, 13, 9, 1, 14, 10])]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T07:25:43.040466Z",
          "iopub.execute_input": "2024-04-03T07:25:43.040819Z",
          "iopub.status.idle": "2024-04-03T07:25:43.191425Z",
          "shell.execute_reply.started": "2024-04-03T07:25:43.040756Z",
          "shell.execute_reply": "2024-04-03T07:25:43.190831Z"
        },
        "trusted": true,
        "id": "leh99HJ128cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean:   {train_sample['target'].mean():.4}\")\n",
        "print(f\"Median: {train_sample['target'].median():.4}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T07:25:45.823418Z",
          "iopub.execute_input": "2024-04-03T07:25:45.823721Z",
          "iopub.status.idle": "2024-04-03T07:25:45.829651Z",
          "shell.execute_reply.started": "2024-04-03T07:25:45.823665Z",
          "shell.execute_reply": "2024-04-03T07:25:45.828697Z"
        },
        "trusted": true,
        "id": "VsYVEa8c28cX",
        "outputId": "a10c0c95-d573-4c4e-bdef-b169d0be9210"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Mean:   6.258\nMedian: 6.031\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import KFold\n",
        "from numpy import random\n",
        "import lightgbm as lgb\n",
        "\n",
        "random.seed(1234)\n",
        "\n",
        "features = ['var_num_peaks_2_denoise_simple','var_percentile_roll50_std_20','var_mfcc_mean4',  'var_mfcc_mean18']\n",
        "target = train_sample[\"target\"].values\n",
        "\n",
        "train_X = train_sample[features].values\n",
        "test_X = test[features].values\n",
        "\n",
        "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\n",
        "oof = np.zeros(len(train_X))\n",
        "prediction = np.zeros(len(submission))\n",
        "\n",
        "n_fold = 3\n",
        "\n",
        "kf = KFold(n_splits=n_fold, shuffle=True, random_state=1337)\n",
        "kf = list(kf.split(np.arange(len(train_sample))))\n",
        "\n",
        "for fold_n, (train_index, valid_index) in enumerate(kf):\n",
        "    print('Fold', fold_n)\n",
        "\n",
        "    trn_data = lgb.Dataset(train_X[train_index], label=target[train_index])\n",
        "    val_data = lgb.Dataset(train_X[valid_index], label=target[valid_index])\n",
        "\n",
        "    params = {'num_leaves': 4,\n",
        "      'min_data_in_leaf': 5,\n",
        "      'objective':'fair',\n",
        "      'max_depth': -1,\n",
        "      'learning_rate': 0.02,\n",
        "      \"boosting\": \"gbdt\",\n",
        "      'boost_from_average': True,\n",
        "      \"feature_fraction\": 0.9,\n",
        "      \"bagging_freq\": 1,\n",
        "      \"bagging_fraction\": 0.5,\n",
        "      \"bagging_seed\": 0,\n",
        "      \"metric\": 'mae',\n",
        "      \"verbosity\": -1,\n",
        "      'max_bin': 500,\n",
        "      'reg_alpha': 0,\n",
        "      'reg_lambda': 0,\n",
        "      'seed': 0,\n",
        "      'n_jobs': 1\n",
        "      }\n",
        "\n",
        "    clf = lgb.train(params, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n",
        "\n",
        "    oof[valid_index] += clf.predict(train_X[valid_index], num_iteration=clf.best_iteration)\n",
        "    prediction += clf.predict(test_X, num_iteration=clf.best_iteration)\n",
        "\n",
        "prediction /= n_fold\n",
        "\n",
        "print('\\nMAE: ', mean_absolute_error(target, oof))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T07:25:50.556948Z",
          "iopub.execute_input": "2024-04-03T07:25:50.557244Z",
          "iopub.status.idle": "2024-04-03T07:25:52.15737Z",
          "shell.execute_reply.started": "2024-04-03T07:25:50.557194Z",
          "shell.execute_reply": "2024-04-03T07:25:52.156632Z"
        },
        "trusted": true,
        "id": "RDFd3BzG28cY",
        "outputId": "681e386c-6129-4c03-e46c-83731f4aa11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Fold 0\nTraining until validation scores don't improve for 1000 rounds.\n[1000]\ttraining's l1: 1.59127\tvalid_1's l1: 1.97283\nEarly stopping, best iteration is:\n[587]\ttraining's l1: 1.66628\tvalid_1's l1: 1.95632\nFold 1\nTraining until validation scores don't improve for 1000 rounds.\n[1000]\ttraining's l1: 1.63939\tvalid_1's l1: 1.86458\nEarly stopping, best iteration is:\n[147]\ttraining's l1: 1.88729\tvalid_1's l1: 1.83689\nFold 2\nTraining until validation scores don't improve for 1000 rounds.\n[1000]\ttraining's l1: 1.62559\tvalid_1's l1: 1.92958\nEarly stopping, best iteration is:\n[391]\ttraining's l1: 1.74828\tvalid_1's l1: 1.90434\n\nMAE:  1.8992052473248857\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission['time_to_failure'] = prediction\n",
        "print(submission.head())\n",
        "submission.to_csv('submission.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-03T07:25:59.302945Z",
          "iopub.execute_input": "2024-04-03T07:25:59.303229Z",
          "iopub.status.idle": "2024-04-03T07:25:59.480997Z",
          "shell.execute_reply.started": "2024-04-03T07:25:59.303188Z",
          "shell.execute_reply": "2024-04-03T07:25:59.480252Z"
        },
        "trusted": true,
        "id": "6IgaKC1i28cY",
        "outputId": "6d9d7ba7-1941-41c4-9dcf-4fbceb724e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "            time_to_failure\nseg_id                     \nseg_00030f         4.188369\nseg_0012b5         5.778676\nseg_00184e         7.223985\nseg_003339        10.427658\nseg_0042cc         7.773074\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}